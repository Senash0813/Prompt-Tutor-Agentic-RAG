{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca1af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f1e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    model=\"openai/gpt-oss-20b\"\n",
    ")\n",
    "\n",
    "# response = llm.invoke(\"Who is the president of Sri-Lanka?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36878f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Engineering\n",
      "September 2024\n",
      "57\n",
      "Growing research suggests that focusing on positive instructions in prompting can be more \n",
      "effective than relying heavily on constraints. This approach aligns with how humans prefer \n",
      "positive instructions over lists of what not to do. \n",
      "Instructions directly communicate the desired outcome, whereas constraints might leave the \n",
      "model guessing about what is allowed. It gives flexibility and encourages creativity within the \n",
      "defined boundaries, while constraints can limit the model’s potential. Also a list of constraints \n",
      "can clash with each other.\n",
      "Constraints are still valuable but in certain situations. To prevent the model from generating \n",
      "harmful or biased content or when a strict output format or style is needed.\n",
      "If possible, use positive instructions: instead of telling the model what not to do, tell it what to \n",
      "do instead. This can avoid confusion and improve the accuracy of the output. \n",
      "DO:\n"
     ]
    }
   ],
   "source": [
    "#from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# pdf_url = \"https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\"\n",
    "\n",
    "# loader = UnstructuredURLLoader(urls=[pdf_url])\n",
    "# documents = loader.load()\n",
    "\n",
    "loader = PyMuPDFLoader(\"Docs/prompt_engineering.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(docs[100].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b157d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/sh30bz8d77z_s4bg0s58rv6r0000gn/T/ipykernel_14566/3954922425.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Users/senash/Desktop/Prompt Tutor - Agentic RAG/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# optional\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Embeddings stored in FAISS vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bfa2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No  \n",
      "AI: Prompt engineering is the art and science of crafting instructions (prompts) that elicit the best possible responses from language models. Below are the core fundamentals a user should master:\n",
      "\n",
      "| # | Fundamental | What It Means | Why It Matters |\n",
      "|---|------------|--------------|---------------|\n",
      "| 1 | **Clarity & Precision** | Use concise, unambiguous language. | Ambiguity leads to inconsistent or irrelevant answers. |\n",
      "| 2 | **Contextualization** | Provide enough background for the model to understand the task. | Models are stateless; they need explicit context. |\n",
      "| 3 | **Task Definition** | State the desired outcome (e.g., “Explain in 3 bullet points”). | Guides the model toward the correct format. |\n",
      "| 4 | **Instruction Hierarchy** | Separate instructions (e.g., “First, list the steps; then explain each”). | Helps the model follow multi-step reasoning. |\n",
      "| 5 | **Iterative Refinement** | Start with a draft prompt, test, then refine. | Small tweaks often yield large quality jumps. |\n",
      "| 6 | **Prompt Chaining** | Use earlier outputs as new prompts to guide further reasoning. | Enables complex, multi‑stage tasks. |\n",
      "| 7 | **Prompt Templates** | Create reusable templates for recurring tasks. | Saves time and ensures consistency. |\n",
      "| 8 | **Prompt Length & Token Limits** | Stay within token limits for your model’s context window. | Prevents truncation or loss of information. |\n",
      "| 9 | **Evaluation & Feedback Loops** | Compare outputs, rank, and iterate. | Turns subjective “good” into objective metrics. |\n",
      "|10 | **Ethical & Safety Considerations** | Avoid harmful or biased prompts. | Protects users and upholds responsible AI. |\n",
      "\n",
      "### Quick Starter Guide\n",
      "\n",
      "1. **Identify the Goal**  \n",
      "   *What do you want the model to do?*  \n",
      "   Example: “Summarize the following article in 200 words.”\n",
      "\n",
      "2. **Provide Context**  \n",
      "   *Give the model enough data.*  \n",
      "   Example: “Here is the article: …”\n",
      "\n",
      "3. **Specify the Format**  \n",
      "   *Tell the model how to output.*  \n",
      "   Example: “Answer in bullet points.”\n",
      "\n",
      "4. **Add Constraints**  \n",
      "   *Limits, tone, audience.*  \n",
      "   Example: “Use simple language for high school students.”\n",
      "\n",
      "5. **Iterate**  \n",
      "   *Run, evaluate, tweak.*\n",
      "\n",
      "### Example Prompt\n",
      "\n",
      "> **Prompt:**  \n",
      "> “I have a 500‑word essay on climate change. Summarize it in 3 bullet points, each no more than 15 words, and use a friendly tone suitable for a 12‑year‑old reader. Avoid technical jargon.”\n",
      "\n",
      "### Common Pitfalls\n",
      "\n",
      "| Pitfall | Fix |\n",
      "|---------|-----|\n",
      "| Overly vague prompts | Add specificity |\n",
      "| Too long prompts | Split into multiple prompts |\n",
      "| Ignoring token limits | Count tokens or shorten |\n",
      "\n",
      "### Resources for Deeper Learning\n",
      "\n",
      "- **OpenAI Cookbook** – Prompt design examples  \n",
      "- **Prompt Engineering Guide** by Prompt Engineering Lab  \n",
      "- **AI Prompt Design Course** (Coursera, Udacity)\n",
      "\n",
      "---\n",
      "\n",
      "**Takeaway:** Prompt engineering is about *communicating* your intent clearly and iteratively. Master these fundamentals, experiment, and you'll consistently get high‑quality outputs from language models. Happy prompting!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt engineering is the art and science of crafting instructions (prompts) that elicit the best possible responses from language models. Below are the core fundamentals a user should master:\n",
      "\n",
      "| # | Fundamental | What It Means | Why It Matters |\n",
      "|---|------------|--------------|---------------|\n",
      "| 1 | **Clarity & Precision** | Use concise, unambiguous language. | Ambiguity leads to inconsistent or irrelevant answers. |\n",
      "| 2 | **Contextualization** | Provide enough background for the model to understand the task. | Models are stateless; they need explicit context. |\n",
      "| 3 | **Task Definition** | State the desired outcome (e.g., “Explain in 3 bullet points”). | Guides the model toward the correct format. |\n",
      "| 4 | **Instruction Hierarchy** | Separate instructions (e.g., “First, list the steps; then explain each”). | Helps the model follow multi-step reasoning. |\n",
      "| 5 | **Iterative Refinement** | Start with a draft prompt, test, then refine. | Small tweaks often yield large quality jumps. |\n",
      "| 6 | **Prompt Chaining** | Use earlier outputs as new prompts to guide further reasoning. | Enables complex, multi‑stage tasks. |\n",
      "| 7 | **Prompt Templates** | Create reusable templates for recurring tasks. | Saves time and ensures consistency. |\n",
      "| 8 | **Prompt Length & Token Limits** | Stay within token limits for your model’s context window. | Prevents truncation or loss of information. |\n",
      "| 9 | **Evaluation & Feedback Loops** | Compare outputs, rank, and iterate. | Turns subjective “good” into objective metrics. |\n",
      "|10 | **Ethical & Safety Considerations** | Avoid harmful or biased prompts. | Protects users and upholds responsible AI. |\n",
      "\n",
      "### Quick Starter Guide\n",
      "\n",
      "1. **Identify the Goal**  \n",
      "   *What do you want the model to do?*  \n",
      "   Example: “Summarize the following article in 200 words.”\n",
      "\n",
      "2. **Provide Context**  \n",
      "   *Give the model enough data.*  \n",
      "   Example: “Here is the article: …”\n",
      "\n",
      "3. **Specify the Format**  \n",
      "   *Tell the model how to output.*  \n",
      "   Example: “Answer in bullet points.”\n",
      "\n",
      "4. **Add Constraints**  \n",
      "   *Limits, tone, audience.*  \n",
      "   Example: “Use simple language for high school students.”\n",
      "\n",
      "5. **Iterate**  \n",
      "   *Run, evaluate, tweak.*\n",
      "\n",
      "### Example Prompt\n",
      "\n",
      "> **Prompt:**  \n",
      "> “I have a 500‑word essay on climate change. Summarize it in 3 bullet points, each no more than 15 words, and use a friendly tone suitable for a 12‑year‑old reader. Avoid technical jargon.”\n",
      "\n",
      "### Common Pitfalls\n",
      "\n",
      "| Pitfall | Fix |\n",
      "|---------|-----|\n",
      "| Overly vague prompts | Add specificity |\n",
      "| Too long prompts | Split into multiple prompts |\n",
      "| Ignoring token limits | Count tokens or shorten |\n",
      "\n",
      "### Resources for Deeper Learning\n",
      "\n",
      "- **OpenAI Cookbook** – Prompt design examples  \n",
      "- **Prompt Engineering Guide** by Prompt Engineering Lab  \n",
      "- **AI Prompt Design Course** (Coursera, Udacity)\n",
      "\n",
      "---\n",
      "\n",
      "**Takeaway:** Prompt engineering is about *communicating* your intent clearly and iteratively. Master these fundamentals, experiment, and you'll consistently get high‑quality outputs from language models. Happy prompting!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import ZeroShotAgent, AgentType, initialize_agent\n",
    "from tools import retriever_tool, prompt_generator_tool, query_classifier_tool\n",
    "\n",
    "\n",
    "tools = [retriever_tool, prompt_generator_tool, query_classifier_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "\n",
    "def query_pipeline(user_query):\n",
    "    result = agent({\"input\": user_query, \"chat_history\": []})\n",
    "    return result[\"output\"]\n",
    "\n",
    "user_query = \"What are the fundamentals a user should know about prompt engineering?\"\n",
    "print(query_pipeline(user_query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
