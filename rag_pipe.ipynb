{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca1af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f1e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# response = llm.invoke(\"Who is the president of Sri-Lanka?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36878f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Engineering\n",
      "September 2024\n",
      "57\n",
      "Growing research suggests that focusing on positive instructions in prompting can be more \n",
      "effective than relying heavily on constraints. This approach aligns with how humans prefer \n",
      "positive instructions over lists of what not to do. \n",
      "Instructions directly communicate the desired outcome, whereas constraints might leave the \n",
      "model guessing about what is allowed. It gives flexibility and encourages creativity within the \n",
      "defined boundaries, while constraints can limit the modelâ€™s potential. Also a list of constraints \n",
      "can clash with each other.\n",
      "Constraints are still valuable but in certain situations. To prevent the model from generating \n",
      "harmful or biased content or when a strict output format or style is needed.\n",
      "If possible, use positive instructions: instead of telling the model what not to do, tell it what to \n",
      "do instead. This can avoid confusion and improve the accuracy of the output. \n",
      "DO:\n"
     ]
    }
   ],
   "source": [
    "#from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# pdf_url = \"https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\"\n",
    "\n",
    "# loader = UnstructuredURLLoader(urls=[pdf_url])\n",
    "# documents = loader.load()\n",
    "\n",
    "loader = PyMuPDFLoader(\"Docs/prompt_engineering.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(docs[100].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b157d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/sh30bz8d77z_s4bg0s58rv6r0000gn/T/ipykernel_1862/3954922425.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Users/senash/Desktop/Prompt Tutor - Agentic RAG/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# optional\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Embeddings stored in FAISS vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import ZeroShotAgent, AgentType, initialize_agent\n",
    "from tools import retriever_tool, prompt_generator_tool, query_classifier_tool\n",
    "\n",
    "\n",
    "tools = [retriever_tool, prompt_generator_tool, query_classifier_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "def is_prompt_engineering_related(query):\n",
    "    \"\"\"Check if query is related to prompt engineering\"\"\"\n",
    "    classification_prompt = \"\"\"\n",
    "    Determine if the following query is related to prompt engineering, AI prompts, or creating prompts.\n",
    "    Respond with only 'YES' or 'NO'.\n",
    "    \n",
    "    Query: {query}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    result = llm.invoke(classification_prompt.format(query=query))\n",
    "    return result.content.strip().upper() == 'YES'\n",
    "\n",
    "def query_pipeline(user_query):\n",
    "    if not is_prompt_engineering_related(user_query):\n",
    "        return \"I'm sorry, but I can only help with prompt engineering related questions. Please ask about creating prompts, prompt techniques, or prompt optimization.\"\n",
    "    \n",
    "    result = agent({\"input\": user_query, \"chat_history\": []})\n",
    "    return result[\"output\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is prompt engineering ?\"\n",
    "print(query_pipeline(user_query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
