{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca1af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f1e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    model=\"openai/gpt-oss-20b\"\n",
    ")\n",
    "\n",
    "# response = llm.invoke(\"Who is the president of Sri-Lanka?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36878f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# pdf_url = \"https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\"\n",
    "\n",
    "# loader = UnstructuredURLLoader(urls=[pdf_url])\n",
    "# documents = loader.load()\n",
    "\n",
    "loader = PyMuPDFLoader(\"Docs/prompt_engineering.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(docs[100].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b157d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# optional\n",
    "# vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Embeddings stored in FAISS vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bfa2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Response ======\n",
      "**Prompt engineering** is the iterative process of crafting, refining, and optimizing the input (prompt) given to a large language model so that it predicts the desired output accurately. It involves selecting the right words, style, tone, structure, and context, as well as choosing the appropriate model and configurations. The goal is to design highâ€‘quality prompts that guide the LLM to produce clear, accurate, and relevant responses.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import Document\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def parser(llm_output):\n",
    "    lines = llm_output.strip().split(\"\\n\")\n",
    "    \n",
    "    structured_output = {\n",
    "        \"header\": \"====== Response ======\",\n",
    "        \"content\": [line.strip() for line in lines if line.strip()],  \n",
    "    }\n",
    "    return structured_output\n",
    "\n",
    "def query_pipeline(user_query):\n",
    "    retrieved_docs = retriever.get_relevant_documents(user_query)\n",
    "    \n",
    "    # Combine the retrieved documents into a single context\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Pass the context and query to the LLM chain\n",
    "    llm_output = llm_chain.run({\"context\": context, \"question\": user_query})\n",
    "    \n",
    "    # Parse the LLM output\n",
    "    parsed_output = parser(llm_output)\n",
    "    \n",
    "    return parsed_output\n",
    "\n",
    "user_query = \"What is prompt engineering?\"\n",
    "response = query_pipeline(user_query)\n",
    "\n",
    "\n",
    "print(response[\"header\"])\n",
    "for line in response[\"content\"]:\n",
    "    print(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
